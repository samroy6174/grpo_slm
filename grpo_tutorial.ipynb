{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd6a711",
   "metadata": {},
   "source": [
    "# GRPO Tutorial Notebook\n",
    "\n",
    "This notebook reproduces the code frames from the video tutorial on Group Relative Policy Optimization (GRPO) for small reasoning LMs. Follow each cell to build the environment, model, loss functions, and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe7377",
   "metadata": {},
   "source": [
    "## 1. Install & Imports\n",
    "\n",
    "Install the package in editable mode and import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c643c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e .\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from grpo_slm.env import ReasoningEnv\n",
    "from grpo_slm.model import ReasoningModel\n",
    "from grpo_slm.grpo import ppo_loss, grpo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee9414",
   "metadata": {},
   "source": [
    "## 2. Define and Test the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and test the environment\n",
    "env = ReasoningEnv(max_value=10)\n",
    "prompt = env.reset()\n",
    "print(\"Prompt:\", prompt)\n",
    "# Choose a random action to test step()\n",
    "action = env.action_space.sample()\n",
    "_, reward, done, info = env.step(action)\n",
    "print(f\"Test step -> action: {action}, reward: {reward}, true answer: {info['true']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c278c",
   "metadata": {},
   "source": [
    "## 3. Instantiate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f85bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ReasoningModel('gpt2', device=device)\n",
    "\n",
    "# Test generation\n",
    "response = model.generate([prompt], max_new_tokens=5)[0]\n",
    "print(\"Generated response:\", response)\n",
    "\n",
    "# Test log_probs\n",
    "logp = model.log_probs([prompt], [response])[0]\n",
    "print(\"Log-prob of the response:\", logp.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f384113",
   "metadata": {},
   "source": [
    "## 4. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for loss demonstration\n",
    "old = torch.tensor([1.0, 2.0, 3.0])\n",
    "new = torch.tensor([1.2, 1.8, 2.5])\n",
    "adv = torch.tensor([0.5, -0.2, 1.0])\n",
    "\n",
    "# Compute PPO and GRPO losses\n",
    "print(\"PPO loss:\", ppo_loss(old, new, adv).item())\n",
    "print(\"GRPO loss:\", grpo_loss(old, new, adv).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895df68a",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple one-epoch training demonstration\n",
    "optimizer = torch.optim.Adam(model.model.parameters(), lr=1e-5)\n",
    "batch_size = 4\n",
    "\n",
    "prompts, responses, rewards, old_logps = [], [], [], []\n",
    "for _ in range(batch_size):\n",
    "    p = env.reset()\n",
    "    r = model.generate([p], max_new_tokens=5)[0]\n",
    "    lp = model.log_probs([p], [r])[0]\n",
    "    _, rew, _, _ = env.step(int(r.strip()) if r.strip().isdigit() else -1)\n",
    "    prompts.append(p); responses.append(r); rewards.append(rew); old_logps.append(lp)\n",
    "\n",
    "rewards = torch.tensor(rewards, device=device)\n",
    "old_logps = torch.stack(old_logps)\n",
    "adv = rewards - rewards.mean()\n",
    "new_logps = model.log_probs(prompts, responses)\n",
    "loss = grpo_loss(old_logps, new_logps, adv)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Training step -> loss: {loss.item():.4f}, avg reward: {rewards.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2646bda",
   "metadata": {},
   "source": [
    "## 6. Plotting Reward Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards distribution\n",
    "plt.hist(rewards.cpu().numpy())\n",
    "plt.title(\"Reward Distribution\")\n",
    "plt.xlabel(\"Reward\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
